\documentclass[kulak]{tplt} 
% options: kulak (default) or kul

\title{Algebraic Methods in Combinatorics}
\author{Assignment 2 - Solutions}
\date{Summer semester 2022 -- 2023}
\address{
	\textbf{Max Planck Institute for the Mathematics in the Sciences} \\
	\textbf{Universit\"at Leipzig}}


\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{bbold}
\usepackage{listings}
\usepackage{lineno}
%\usepackage[margin=3cm]{geometry}
\usepackage[all,cmtip, color,matrix,arrow]{xy}
%\usepackage{amsaddr}
\usepackage{tikz-cd}
\usepackage{amsmath}%To use \text 
\usepackage[utf8]{inputenc}
\usepackage{hyperref}
\usepackage[capitalize]{cleveref}
\crefname{thm}{Theorem}{Theorems}
%\usepackage{bbold}
\usepackage[export]{adjustbox}
\usepackage{todonotes}
\usepackage{bm}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{mathtools}
\usepackage{aliascnt}
\newaliascnt{eqfloat}{equation}
\newfloat{eqfloat}{h}{eqflts}
\floatname{eqfloat}{Equation}
\usepackage{dirtytalk}
\usepackage[mathscr]{euscript}


%\def\shuffle{\sqcup\mathchoice{\mkern-7mu}{\mkern-7mu}{\mkern-3.2mu}{\mkern-3.8mu}\sqcup\,}
\newcommand{\qshuffle}{\overline{\shuffle}}


\theoremstyle{definition}
\newtheorem{thm}{Theorem}[section]
\newtheorem{prop}[thm]{Proposition}
\newtheorem{lm}[thm]{Lemma}
\newtheorem{cor}[thm]{Corollary}
\newtheorem{obs}[thm]{Observation}
\newtheorem{defin}[thm]{Definition}
\newtheorem{smpl}[thm]{Example}
\newtheorem{quest}[thm]{Question}
\newtheorem{exe}[thm]{Exercise}
\newtheorem{const}[thm]{Construction}
\newtheorem{prob}[thm]{Problem}
\newtheorem{conj}[thm]{Conjecture}
\newtheorem{rem}[thm]{Remark}
\crefname{lm}{Lemma}{Lemmas}
\crefname{thm}{Theorem}{Theorems}
\crefname{prop}{Proposition}{Propositions}
\crefname{defin}{Definition}{Definitions}
\crefname{rem}{Remark}{Remarks}

\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\PP}{\mathbb{P}}

\newcommand{\one}{\mathbb{1}}

\newcommand{\CC}{\mathcal C}
\newcommand{\JJ}{\mathcal J}
\newcommand{\II}{\mathcal I}
\newcommand{\BB}{\mathcal B}
\newcommand{\FF}{\mathcal F}

%vectors
\newcommand{\vv}{\mathsf{v}}
\newcommand{\vw}{\mathsf{w}}
\newcommand{\vj}{\mathsf{j}}
\newcommand{\vx}{\mathsf{x}}
\newcommand{\vy}{\mathsf{y}}
\newcommand{\va}{\mathsf{a}}
\newcommand{\vc}{\mathsf{c}}

\newcommand{\spn}{\mathrm{span}}
\newcommand{\rowspn}{\mathrm{rowspan}}
\newcommand{\rk}{\mathrm{rk}}
\newcommand{\tr}{\mathrm{tr}}
\newcommand{\conv}{\mathrm{conv}}
\newcommand{\maxcut}{\mathrm{maxcut}}
\newcommand{\Tr}{\mathrm{Tr}}
\newcommand{\we}{\mathrm{we}}
\newcommand{\Id}{\mathrm{Id}}
\newcommand{\spec}{\mathrm{spec}}


\begin{document}

\maketitle
\begin{enumerate}
\item 
Fix an integer $n$, so that $\chi_G(n)$ is the number of proper colourings of $G$ using $n$ colours.
Let $e = \{v_1, v_2\}$.
Fix $f$ a proper colouring of $G\setminus e$.
We have two distinct cases: either $f(v_1) \neq f(v_2)$, in which case $f$ is a proper colouring of $G$, or $f(v_1) = f(v_2)$, in which case this induces $\hat{f}$ a colouring in $G/_e$ by setting $f(e) = f(v_1)$.
This is a proper colouring, and in fact any proper colouring of $G/_e$ can be uniquely obtained in such a way.

This constructs a bijection between
\begin{align*}
 \{ \text{ proper colourings of $G$ with $n$ colours }\} &\uplus \{ \text{ proper colourings of $G/_e$ with $n$ colours }\} \\
 &\leftrightarrow \{ \text{ proper coloruings of $G\setminus e$ with $n$ colours }\}\, , 
\end{align*}
thus we conclude that 
$$ \chi_{G\setminus e }(n) = \chi_G(n) + \chi_{G/_e}(n)\, . $$

\item 
\begin{enumerate}
\item 
Any proper colouring of $K_m$ has to use different colours for each vertex of the graph, so $\chi_{K_m}(n) = n(n-1) \cdots (n-m+1) = \frac{n!}{(n-m)!}$.

\item 
A proper colouring of the graph with no edges is simply a map $V(0_m) \to [n]$, so there are $\chi_{0_m}(n) = n^m$ many such colourings.


\item 
We claim that if a tree $T$ has $v$ vertices, then $\chi_T(n) = n(n-1)^v$.
We use induction on $v$.
If $v=1$, then the graph has no edges and so $\chi_T(n) = n^1 = n (n-1)^0$.

For the induction step, let $e$ be a leaf of the tree, so that $T\setminus e$ is the disjoint union of two graphs, an isolated vertex $\{\vv\}$ and some smaller tree $T'$ with $v-1$ vertices.
Therefore, $\chi_{T \setminus e}(n) = n \chi_{T'}(n) = n^2(n-1)^{v-2}$, where we used the induction hypothesis on $T'$.
On the other hand, $T/_e$ is also a tree with $v-1$ vertices, so $\chi_{T/_e}(n) = n (n-1)^{v-2}$.
We conclude with the deletion contraction:
$$\chi_T(n) = \chi_{T'\setminus e}(n) - \chi_{T/_e}(n) = n^2(n-1)^{v-2}  - n(n-1)^{v-2} = n(n-1)^{v-1} \, .  $$


\item 
Let $C_m$ be the cycle on $m$ vertices, and $T_m$ a path on $m$ vertices.
Notice that deleting an edge on $C_{m+1}$ we get $T_{m+1}$, which is a tree, and that the contraction $C_{m+1}/_e$ is isomorphic to $C_m$.
Therefore, we use deletion contraction to get 
$$\chi_{C_{m+1}}(n) =  \chi_{T_{m+1}}(n) - \chi_{C_m}(n) =  n(n-1)^m - \chi_{C_m}(n) \, . $$

By multiplying this equation by $(-1)^m$ and summing for $m=3, \ldots, M$ we get,
\begin{align*}
\sum_{m=3}^M (-1)^m\chi_{C_{m+1}}(n) &= \sum_{m=3}^M (-1)^m n(n-1)^m - \sum_{m=3}^M (-1)^m \chi_{C_{m}}(n) \\
\sum_{m=3}^M (-1)^m(\chi_{C_{m+1}}(n) + \chi_{C_{m}}(n) ) &= n \frac{(n-1)^{M+1}(-1)^{M+1} - (n-1)^3(-1)^3}{-(n-1) -  1}\\
(-1)^M \chi_{C_{M+1}}(n) + (-1)^3 \chi_{C_{3}}(n) &= -(n-1)^{M+1}(-1)^{M+1} + (n-1)^3(-1)^3\\
\chi_{C_{M+1}}(n) + (-1)^{M+3} \chi_{C_{3}}(n) &= -(n-1)^{M+1}(-1)^{2M+1} + (n-1)^3(-1)^{M+3}\\
\chi_{C_{M+1}}(n) &= (n-1)^{M+1} + (n-1)^3(-1)^{M+3} - (-1)^{M+3} \chi_{C_{3}}(n)\\
\chi_{C_{M+1}}(n) &= (n-1)^{M+1} - (-1)^M(n-1)\left( (n-1)^2 - n(n-2) \right) \\
\chi_{C_{M+1}}(n) &= (n-1)^{M+1} - (-1)^M(n-1)
\end{align*}
where we use that $\chi_{C_{3}}(n) = n(n-1)(n-2)$, since $C_{3}$ is the complete graph on $3$ vertices.
We conclude the formula for $M\geq 3$ to be
$$\chi_{C_M}(n) = (n-1)^M + (-1)^M(n-1) \, . $$
\end{enumerate}


\item 
Recall that for any graph $G$, $(A_G^k)_{i, j}$ is the number of paths of length $k$ connecting the vertices $i$ and $j$.
Therefore, one can see that $A_{K_{m, n}}^2 = \begin{pmatrix}
n J_m & 0 \\ 0 & mJ_n
\end{pmatrix}$, where $J_k$ is the $k\times k$ all-one matrix.
The spectrum of this matrix is $\{ 0 ^{\times (m+n-2) }, mn^{\times 2}\}$.
Indeed, any vector $\vv $ such that $\sum_{i=1}^m \vv_i = 0$ and $\sum_{i=m+1}^{m+n} \vv_i = 0$ has $A_{K_{m, n}}^2\vv =0 $ and the vectors $(\underbrace{1}_{m \text{ times}}, \underbrace{0}_{n \text{ times}})$ and  $ (\underbrace{0}_{m \text{ times}}, \underbrace{1}_{n \text{ times}})$ are linearly independent eigenvectors corresponding to the eigenvalue $mn$.

Thus, $0$ is an eigenvalue of $A_{K{m, n}}$ with multiplicity $m+n-2$ and the eigenvalues $\{ \sqrt{mn}, -\sqrt{mn}\}$ have combined multiplicity two.
Because $\Tr A_{K_{m, n}} =0 $, we have that each of these values has multiplicity one.
Thus, the spectrum is 
$$ \{ \sqrt{mn}, \underbrace{0, \ldots, 0}_{m+n-2 \text{ many times }}, -\sqrt{mn} \} \, . $$

\item 
\begin{enumerate}
\item 
First we observe that $\vv_1 = \frac{1}{\sqrt{n}} \mathbb{1}$ is indeed an orthonormal eigenvector.
Note that for each vertex $i$ we have $(A_G \vv_1)_i = \sum_{j : i\-- j} \frac{1}{\sqrt{n}} = \frac{d}{\sqrt{n}}$, as $i$ has exactly $d$ neighbours.
This concludes that it is an eigenvector.
The orthonormality is imediate.

Assume now that there is some eigenvalue $\lambda > d$, and let $\vv $ be its corresponding eigenvector, let $i$ be the vertex such that $|\vv_i|$ is maximal.
Then 

$$\lambda |\vv_i | = |(A_G \vv)_i| = \left|\sum_{j : i\-- j} \vv_j \right| \leq \sum_{j : i\-- j} |\vv_j| \leq d |\vv_i| < \lambda |\vv_i | \, ,$$
a contradiction.

\item 
We observe that if $G$ is connected, then $A_G$ is irreducible, that is there is no permutation matrix $P$ such that $PA_GP^{-1} = \begin{pmatrix}
A&B\\ 0 & C\end{pmatrix}$ where $A, C$ are square non-empty matrices.
Thus the exercise follows from Pieron-Frobbenius theorem.
Specifically, any non-negative matrix that is irreducible has a unique largest eigenvalue, with a one-dimensional eigenspace.

\item 
For the direct implication, assume that $G$ is bipartite.
Let the corresponding vertex partition be $V = V_1 \uplus V_2$ and identify $\R^V = \R^{V_1} \times \R^{V_2}$.
Define the map $\phi : \R^{V_1} \times \R^{V_2} \to \R^{V_1} \times \R^{V_2}$ by $\phi(\vv, \vw) \mapsto (\vv, -\vw)$.
This maps the eigenspace of $\lambda $ to $-\lambda$.
Indeed, because $G$ is bipartite, $A_G = \begin{pmatrix}
0 & A\\
A^T & 0
\end{pmatrix}$ so if $(\vv, \vw)$ is in the eigenspace of $A_G$ corresponding to $\lambda$, then $A_G (\vv, \vw)^T = \lambda (\vv, \vw)^T$, that is $(A\vw, A^T\vv) = \lambda (\vv, \vw)^T$, so 
$$A_G \phi(\vv, \vw)^T = A_G (\vv, -\vw)^T = (-A\vw, A^T\vv) = \lambda (-\vv, \vw)^T  = -\lambda \phi(\vv, \vw)^T\, . $$
This concludes that the bijective linear map $\phi$ sends eigenspaces to eigenspaces of its symmetric eigenvalue, so the multiplicities are the same.

For the inverse implication, assume that $\spec \, G = - \spec \, G$.
Therefore we have for any odd number $k$ that $\Tr A_G^k = \sum_{\lambda \in \spec \, G } \lambda^k = 0$.
Recall that for any graph $G$, $(A_G^k)_{i, j}$ is the number of paths of length $k$ connecting the vertices $i$ and $j$.
So $\Tr A_G^k = 0$ is the number of cycles of $G$ of length $k$.
We conclude that $G$ has no cycles of odd length, so it is bipartite.


\item 
First observe that $\overline{G}$ is a regular graph, with common degree $n - 1 - d $.
Therefore $\frac{1}{\sqrt{n}} \mathbb{1}$ is an eigenvalue of $\overline{G}$, from 2a), with the same eigenvector as $G$.
Now let $\vv $ be an eigenvector of $\overline{G}$, corresponding to the eigenvalue $\lambda$, and assume that $\vv \perp \mathbb{1}$, this captures all the remaining eigenvectors because of the spectral theorem.
Then note that $A_{\overline{G}} + A_G = J - \Id_n$, where $J$ is the all-one $n\times n$ matrix.
Note that $J\vv = \vec{0}$.
Thus
$$\lambda \vv = A_{\overline{G}}\vv = (J - \Id_n - A_G)\vv = -\vv - A_G \vv\, ,$$
that is $A_G \vv = -(\lambda + 1)\vv$.
This shows that the eigenspaces are the same, and the corresponding eigenvalues are related by $\lambda \mapsto -(\lambda + 1)$, as desired.
\end{enumerate}


\item 
Let $P$ be the Petersen graph, and let $G$ be the line graph of $K_5$.
Because $K_5$ has $10$ edges, $G$ has $10$ vertices.
Furthermore, each edge in $K_5$ has $6$ neighbouring edges, so $G$ is $6$-regular.
One can notice that $P$ is the complementary graph of $G$, so we compute the eigenvalues of $G$ and use 4.d).

Let $B = B_{K_5}$ be the incidence matrix of $K_5$, that is for $v\in V(K_5)$ and $e\in E(K_5)$, the corresponding entry $B_{v, e}$ is $1$ if $v\in e$, and $0$ otherwise.
In this way we have 
\begin{equation}\label{eq:similar}
\begin{split}
A_{K_5} &= B B^T - 4 \Id_{5} \, , \\
A_G &= B^T B - 2 \Id_{10} \, .
\end{split}
\end{equation}

We will use \eqref{eq:similar} to compute the eigenvalues of $A_G$.
Specifically, we will show that any eigenvalue of $B B^T$ is an eigenvalue of $B^T B$, with at least the same multiplicity, by mapping an eigenspace to another injectively.
Finally, we will show that all remaning eigenvalues of $B^T B$ are zero, by analyzing its rank.
The connection between the spectrum og $G$ and $K_5$ is then done using \eqref{eq:similar}.

We start with the latter.
Recall that if $\mu_{\lambda}$ is the multiplicity of the eigenvalue $\lambda $ in the matrix $A$, $\rk(A -\lambda  \Id_n ) + \mu_{\lambda} \geq n$ with equality for symetric matrices (from the spectral theorem).
It is easy to see that $\rk B^T = 5 $ is maximal: if $e_1, e_2, e_3$ are three edges that form a triangle, then $B_{\cdot, e_1} + B_{\cdot, e_2} - B_{\cdot, e_3}$ is a multiple of a canonical basis element.
Thus, we observe that $\rk(A_G + 2 \Id_{10} ) = \rk (B^T B) \leq \rk B^T = 5$.
Therefore $\mu_{-2} \geq 5$.

Now consider the eigenspace $V_{\lambda} $ of $B B^T$ corresponding to the eigenvalue $\lambda$, and the eigenspace $W_{\lambda} $ of $B^T B$ corresponding to the eigenvalue $\lambda$.
The map $B^T$ is an injective map $\R^5 \to \R^{10}$, as $\rk B^T = 5$.
Furthermore, if $\vv \in V_{\lambda} $, we have $B^T B B^T v = \lambda B^T v$.
So $B^T V_{\lambda} \subseteq W_{\lambda}$.


We now compute the spectrum of each of the matrices.
First, we know that 
$$\spec \, K_5 = \{4^{\times 1 }, -1^ {\times 4} \}\, ,$$
thus
$$\spec \, B B^T = \{8^{\times 1 }, 3^{\times 4} \}\, ,$$
and therefore the eigenspace of $B^T B$ satisfies
$$\spec \, B B^T \subseteq \{8^{\times 1 }, 3^{\times 4}, 0^{\times 5} \}\, ,$$
and since this already has the correct cardinality $10$, this is precisely the spectrum of the matrix.

Thus
$$\spec \, G = \{6^{\times 1 }, 1^{\times 4}, -2^ {\times 5} \}\, ,$$

Therefore, from 4.d)
$$\spec \, P = \{3^{\times 1}, 1^{\times 5}, -2^{\times 4} \}\, .$$
Alternatively, the following python code, yields the spectrum of $A_P$:
\begin{verbatim}
import numpy as np

# Adjacency matrix Peterson graph

A_P = [[0, 1, 0, 0, 1, 1, 0, 0, 0, 0],

      [1, 0, 1, 0, 0, 0, 1, 0, 0, 0],

      [0, 1, 0, 1, 0, 0, 0, 1, 0, 0],

      [0, 0, 1, 0, 1, 0, 0, 0, 1, 0],

      [1, 0, 0, 1, 0, 0, 0, 0, 0, 1],

      [1, 0, 0, 0, 0, 0, 0, 1, 1, 0],

      [0, 1, 0, 0, 0, 0, 0, 0, 1, 1],

      [0, 0, 1, 0, 0, 1, 0, 0, 0, 1],

      [0, 0, 0, 1, 0, 1, 1, 0, 0, 0],

      [0, 0, 0, 0, 1, 0, 1, 1, 0, 0]]

sorted(list(np.round(np.linalg.eig(np.array(A_P))[0])))

>> [-2.0, -2.0, -2.0, -2.0, 1.0, 1.0, 1.0, 1.0, 1.0, 3.0]
\end{verbatim}

\item 
We start by establishing a useful counting lemma:
\begin{lm}\label{lm:binsum}
Let $n> 0$ be an integer, then
$$ \sum_{i=0}^n (-1)^i\binom{n}{i} = 0 \, ,$$
furthermore, if $n=0$, then $\sum_{i=0}^n (-1)^i\binom{n}{i} = 1$.
\end{lm}

\begin{proof}
The case $n=0$ is imediate.
We can therefore assume that $n>0$, and apply the Newton binomial theorem on $(x+y)^n$ for $x = -1$ and $y = 1$, which gives the desired formula. 
\end{proof}

\begin{enumerate}
\item 
Let's prove the four equations.
For the first one let $A \in \binom{E}{i}$ and $B \in \binom{E}{k}$, then
$$(M_{i, j} M_{j, k})_{A, B} = \sum_{C \in \binom{E}{j}} (M_{i, j})_{A, C}( M_{j, k})_{C, B} = \sum_{\substack{C \in \binom{E}{j} \\ A \subseteq C \subseteq B }} 1\, ,$$
which is equal to $\binom{k-i}{j-i}$ whenever $A\subseteq B$, and is zero otherwise, so
$M_{i, j} M_{j, k} = \binom{k-i}{j-i}M_{i, k}$.

For the second equation let $A \in \binom{E}{i}$ and $B \in \binom{E}{k}$, then
$$(M_{i, j} N_{j, k})_{A, B} = \sum_{C \in \binom{E}{j}} (M_{i, j})_{A, C}( N_{j, k})_{C, B} = \sum_{\substack{C \in \binom{E}{j} \\ A \subseteq C \\ C \cap B = \emptyset }} 1 \, ,
$$
which is equal to $\binom{e-k-i}{j-i}$ whenever $A\cap B = \emptyset $, and is zero otherwise, so
$M_{i, j} N_{j, k} = \binom{e-k-i}{j-i}N_{i, k}$.

For the third equation let $A \in \binom{E}{j}$ and $B \in \binom{E}{k}$, then
\begin{align*}
\left(\sum_{i=0}^j (-1)^i M_{i, j}^T N_{i, k}\right)_{A, B} &= \sum_{i=0}^j (-1)^i \sum_{C \in \binom{E}{i}} (M_{i, j}^T)_{A, C}( N_{i, k})_{C, B}\\
&=\sum_{i=0}^j (-1)^i \sum_{C \in \binom{E}{i}} (M_{i, j})_{C, A}( N_{i, k})_{C, B}\\
&=\sum_{i=0}^j (-1)^i \sum_{\substack{C \in \binom{E}{i}\\ C \subseteq A \\ C \cap B = \emptyset  }} 1 =\sum_{i=0}^j (-1)^i \binom{|A \setminus B|}{i}
\end{align*}
where we note that $\binom{|A \setminus B|}{i} = 0 $ for $i > |A \setminus B|$.
Therefore, from \cref{lm:binsum} this is equal to $1$ whenever $A \subseteq B $, and is zero otherwise, so we can conclude that
$\sum_{i=0}^j (-1)^i M_{i, j}^T N_{i, k} = M_{j, k}$.


For the fourth equation let $A \in \binom{E}{j}$ and $B \in \binom{E}{k}$, then
\begin{align*}
\left(\sum_{i=0}^j (-1)^i M_{i, j}^T M_{i, k}\right)_{A, B} &= \sum_{i=0}^j (-1)^i \sum_{C \in \binom{E}{i}} (M_{i, j}^T)_{A, C}( M_{i, k})_{C, B}\\
&=\sum_{i=0}^j (-1)^i \sum_{C \in \binom{E}{i}} (M_{i, j})_{C, A}( M_{i, k})_{C, B}\\
&=\sum_{i=0}^j (-1)^i \sum_{\substack{C \in \binom{E}{i}\\ C \subseteq A \\ C\subseteq B}} 1 =\sum_{i=0}^j (-1)^i \binom{|B \cap A|}{i}
\end{align*}
where we note that $\binom{|B \cap A|}{i} = 0 $ for $i > |B \cap A|$.
Therefore, from \cref{lm:binsum} this is equal to $1$ whenever $B \cap A = \emptyset $, and is zero otherwise, so we can conclude that
$\sum_{i=0}^j (-1)^i  M_{i, j}^T M_{i, k} = N_{j, k}$.




\item 
Note that the matrix $N_{i, j}$ is symetric, so that $N_{i, j} = N_{j, i}^T$.
Therefore, we can use equations one and three from 6 a) to get
\begin{align*}
M_{i, r} \left( \sum_{j=0}^i \frac{(-1)^j}{\binom{e-i-j}{r-i}} N_{r, j} M_{j, i}\right) &= \sum_{j=0}^i \frac{(-1)^j}{\binom{e-i-j}{r-i}}M_{i, r} N_{r, j} M_{j, i} = \sum_{j=0}^i \frac{(-1)^j}{\binom{e-i-j}{r-i}}\binom{e-i-j}{r-i} N_{i, j} M_{j, i}\\
&= \sum_{j=0}^i (-1)^j N_{j, i}^T M_{j, i} = \sum_{j=0}^i (-1)^j \left(M_{j, i}^T N_{j, i} \right)^T = M_{i, i}^T = \Id \, ,
\end{align*}
we conclude that $M_{i, r}$ has a right inverse, so it must correspond to an injective map, therefore $\rk M_{i, r} = \binom{e}{i}$.

\item 

\begin{lm}\label{lm:rowspan}
If $A, B, C$ are matrices such that $A = BC$ then $\rowspn \, A \subseteq \rowspn \, C$
\end{lm}

\begin{proof}
Let $\va_1, \ldots , \va_k$ be the row vectors in $A$, and $\vc_1, \ldots, \vc_j$ be the row vectors in $C$.
Write $B = \begin{pmatrix}
b_{i, j}
\end{pmatrix}_{i, j}$.
The matrix equation $A = BC$ translates to
$$ \va_i = \sum_l b_{i, l} \vc_l \, ,$$
so $\va_i \in \spn \{\vc_l\}_l$.
\end{proof}

From the first equation in 6. a), we have that $M_{i, i+1} M_{i+1, r} = (r-i)M_{i, r}$ so from \cref{lm:rowspan} $V^{i} \subseteq V^{i+1}$. 

\item
Using the second equation in 6. a) and from \cref{lm:rowspan}, we have that $\rowspn \,N_{i, r} \subseteq \rowspn \, N_{i+1, r}$.
From the third equation in 6. a), setting $k = r$ we have that $\rowspn \, M_{j, r} \subseteq \cup_{i=0}^j \rowspn \, N_{i, r} = N_{j, r}$
From the fourth equation in 6. a), setting $k = r$ we have that $\rowspn \, N_{j, r} \subseteq \cup_{i=0}^j \rowspn \, M_{i, r} = M_{j, r}$

We conclude that both rowspans are the same.


\item
Recall that $A_G = N_{r, r}$.
If $\vv \in V^i$, then ..

\item 
From 6.a), we have that $\dim (V^i \cap (V^{i+1})^{\perp}) = \binom{e}{i}-\binom{e}{i-1}$.
From 6.d) we have that $(V^i \cap (V^{i+1})^{\perp}) \subseteq V_{\lambda_i}$ is contained in the eigenspace of $A_{K(E, r)}$ corresponding to $(-1)^i\binom{e-r-i}{r-i} = \lambda_i$.

We have $\binom{e}{r} \geq \sum_{i=0}^r \dim V_{\lambda_i} \geq \sum_{i=0}^r \binom{e}{i}-\binom{e}{i-1} = \binom{e}{r}$.
So we have equality all throughout, so $(V^i \cap (V^{i+1})^{\perp}) = V_{\lambda_i}$ and these are indeed all the eigenvalues of $K(E, r)$, so
$$\spec \, K(E, r) = \{  \lambda_i^{\times \binom{e}{i} - \binom{e}{i-1}} | i = 0, \ldots, r\} \, . $$
\end{enumerate}

\end{enumerate}
\end{document}
